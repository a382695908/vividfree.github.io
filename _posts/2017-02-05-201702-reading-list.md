---
layout: post
title: 201702 阅读笔记
category: 阅读笔记
tagline: 
tags: [阅读笔记, LSTM, Attention, machine translation, zero-shot translation, 机器翻译]
theme :
  name : bootstrap-3
---
{% include JB/setup %}

## 技术方面

2016年下半年Google上线了新一代机器翻译系统（GNMT）——基于神经网络的机器翻译系统（Neural Machine Translation, NMT），它应该正在逐步取代原先的基于短语的统计机器翻译系统（Phrased-based Machine Translation, PBMT）。推出GNMT系统后不久，Google先后发表了2篇文章以介绍GNMT中用到的技术。之前就有同事跟我提过这些文章，但因为工作中业务较多，直到这个假期才有空细细学习。和网上很多人的看法大致相同，在DL方面没有特别新的技术创新点，但其整合了近几年DL/RNN的最新成果，同时还凭借Google的TPU硬件和在效果和性能上各种权衡的高效工程实现，得以推出一款实用的机器翻译系统。很高兴能看到论文不仅引用了刘洋师兄发表的直接优化评估指标（特别是非连续的评估指标）的"Minimum risk training for neural machine translation"和涂兆鹏师兄发表的考虑翻译覆盖率的"Coverage-based neural machine translation"，而且GNMT系统在研发时也借鉴了其中的思路。

+ [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)
  - 介绍GNMT的核心实现方案
  - 在GNMT发布之前，NMT经常被提到3大问题：1）training和inference速度均较慢；2）对罕见词或者OOV的处理不好；3）在翻译时存在丢词的可能。为了解决这些问题，Google吸收了近几年DL的最新成果，同时凭借其超强的工程技术能力，推出了一款实用的机器翻译系统。
  - 在整体的系统框架上，还是LSTM RNNs，包括3大块：encoder network, decoder network和attention network。
  - Google通过一些实验也验证了先前论文的结论——神经网络的层数越深，模型的效果越好。当然为了达到这目标，在普通的stacked LSTM上需要加上residual connections，否则会因为exploding and vanishing gradient问题，在6层左右效果达到最好，层数再加深时效果反而会变差。
  - 在attention network方面，用上普通的含有单个隐含层的神经网络，外加softmax层。
  - 在模型训练方面，考虑到计算量大，同时用上了模型并行和数据并行。每个replica使用融合Adam和SGD后的异步更新算法，外加mini-batch。
  - 在处理OOV这个细节问题上，使用wordpiece model。这个模型单独训的，个人觉得也没必要jointly training，因为单独训的话可以使用丰富的单语语料。给定单语语料和一份定好的token集合，基于MLE即可得到一个wordpiece model。
  - 在优化目标方面，最大化给定input计算ground-truth output的概率这种目标与最终的翻译质量评估指标并不完全吻合，所以提出expected reward objective，同时融合MLE和expected reward objective，以期在max likelihood和BLEU（GLEU）上达到平衡。

+ [Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation](https://arxiv.org/abs/1611.04558)
  - 在GNMT系统方案的基础上，如何实现多语言翻译和zero-shot translation。作者提出了一个简洁的方案就是在源语言句子中增加一个表示目标语言的token。
  - 为此作者做了充分的实验
    + one to many, many to one, many to many
    + 合并全部双语语料，在诸多语言对上只训练一个模型。这个好处可想而知，减少训练开销，减少线上加载模型的内存开销。
    + zero-shot translation
    + 当新语料到来时，用新语料是否还会提高效果。合并数据后重训模型 or incremental training。在作者的试验中，合并数据后重训模型的效果更好。
    + 对于前3项的实验，作者的方案虽然简单但都是有效的。
  - 可视化翻译效果，对属于同一语系的相似语言，可以看到一些纯正的cluster。
  - mixed language。这个也算是作者在玩语言玩数据了
  - 个人觉得这篇文章虽然idea不是特别高深，但其实验结果的价值还是蛮大的，也许能引起语言学界特别翻译界的学者的后续深入研究。也许在寻找interlingua这个问题上，这种方法这种思路是不错的。


* * *

**原创文章，转载请注明：**转载自[{{ site.title }}]({{ site.production_url }})

**本文链接地址：**[{{ page.title }}]({{ site.production_url }}{{ page.url }})

* * *
