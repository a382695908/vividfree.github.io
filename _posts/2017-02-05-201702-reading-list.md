---
layout: post
title: 201702 阅读笔记
category: 阅读笔记
tagline: 
tags: [阅读笔记, LSTM, Attention, machine translation, zero-shot translation, 机器翻译, Deep Learning, 深度学习]
theme :
  name : bootstrap-3
---
{% include JB/setup %}

## 循环神经网络 (Recurrent Neural Network, RNN)

虽然早之前就了解了RNN和LSTM的网络结构和优缺点，但一直有些疑问，比如LSTM为啥长这样，RNN网络结构都经历过什么样的演进。最近碰到2篇文章，"LSTM: A Search Space Odyssey"和"A Critical Review of Recurrent Neural Networks for Sequence Learning"，它们在一定程度上解答了那些疑问。

+ [LSTM: A Search Space Odyssey](https://arxiv.org/abs/1503.04069)
  - 第4作者Jurgen Schmidhuber是LSTM的发明者。16年纽约时报曾对他做过一次采访  '[When A.I. Matures, It May Call Jürgen Schmidhuber ‘Dad’](https://www.nytimes.com/2016/11/27/technology/artificial-intelligence-pioneer-jurgen-schmidhuber-overlooked.html)'，机器之心曾经翻译过这篇文章 '[LSTM之父Jürgen Schmidhuber为何名声不显？](http://it.sohu.com/20161128/n474324689.shtml)'。作者在16年还参加了reddit 的Ask Me Anything（AMA）'[I am Jürgen Schmidhuber, AMA!](https://www.reddit.com/r/MachineLearning/comments/2xcyrl/i_am_j%C3%BCrgen_schmidhuber_ama/)'。
  - 这是一篇实验性的文章，介绍vanilla LSTM及其变种，并在3份语料上做详细的对比实验以说明网络结构中哪些是重要的部件。
  - 作者描述的vanilla LSTM并不是LSTM最早提出时的样子。在这篇文章里，vanilla LSTM可以被视为是LSTM的大框架，其他各种LSTM都只是其中的变种，比如对各个gate是否有peepholes connections，是否有forget gate，是否用的是full gradient。
  - 经过详细的实验对比，作者得到一些结论：
    + Vanilla LSTM能得到较好的效果，其他各种变种的效果并没有显著的超越它
    + 合并input gate和forget gate不会显著的降低效果
    + 去掉peepholes connections也不会显著的降低效果。现在看到的很多关于LSTM的文章，大多没有peepholes connections。
    + Forget gate和output activation function是LSTM的重要组件。如果没有forget gate，模型效果会严重下降。
    + 学习率和网络大小的tuning是很重要的。

+ [A Critical Review of Recurrent Neural Networks for Sequence Learning](https://arxiv.org/abs/1506.00019)
  - 这是一篇深入分析RNN和LSTM的文章，用语言描述了网络结构特点和网络部件的作用。
  - 第1节4个问题是有趣的自问自答，像第2个问题，既然是处理时序问题，为啥不用Markov模型？作者在回答时提到经典的HMM模型，从参数规模和模型表达能力上做了对比。个人认为在这块对比上，其实是生成式模型（以HMM模型为例）和判别式模型（以RNN为例）的对比，很多时候判别式模型本来就更为强大更为灵活。
  - 文章还简单提了下 neural Turing machines。读研时就对Turing machines的机制没理解透，这里更是对neural Turing machines做描述，更是木有理解。以后有时间的话多了解下这块。
  - 文章还介绍了LSTM RNNs在机器翻译中的应用，这里并没有给出LSTM RNNs用在机器翻译问题中的损失函数的形式化表达式，自己之前也没看到过，幸好凭借读研时在机器翻译的一点积累，自己还是可以准确的给出损失函数的形式化表达式。相比传统的基于短语的统计机器翻译（PBMT），基于神经网络的机器翻译系统（NMT）的损失函数表达式会更简洁些（表达式更简洁，并不是说计算量更少），因为NMT大体的框架是先完成源语言句子的encode之后再开始decode出目标语言句子，而PBMT做不到这样。

## 机器翻译 (Machine Translation, MT)

2016年下半年Google上线了新一代机器翻译系统（GNMT）——基于神经网络的机器翻译系统（Neural Machine Translation, NMT），它应该正在逐步取代原先的基于短语的统计机器翻译系统（Phrased-based Machine Translation, PBMT）。推出GNMT系统后不久，Google先后发表了2篇文章以介绍GNMT中用到的技术。之前就有同事跟我提过这些文章，但因为工作中业务较多，直到这个假期才有空细细学习。和网上很多人的看法大致相同，这2篇论文在DL方面没有特别高深的技术创新点，但其整合了近几年DL/RNN的最新成果，同时还凭借Google的TPU硬件和在效果和性能上各种权衡的高效工程实现，得以推出一款实用的机器翻译系统。另外也给了想做机器翻译产品的公司一条明路，NMT这条路不但可以走通，而且可以走得更好。

很高兴能看到论文不仅引用了刘洋师兄发表的直接优化评估指标（特别是非连续的评估指标）的"Minimum risk training for neural machine translation"和涂兆鹏师兄发表的考虑翻译覆盖率的"Coverage-based neural machine translation"，而且GNMT系统在研发时也借鉴了其中的思路。

+ [Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation](https://arxiv.org/abs/1609.08144)
  - 介绍GNMT的核心实现方案
  - 在GNMT发布之前，NMT经常被提到3大问题：1）training和inference速度均较慢；2）对罕见词或者OOV的处理不好；3）在翻译时存在丢词的可能。为了解决这些问题，Google吸收了近几年DL的最新成果，同时凭借其超强的工程技术能力，推出了一款实用的机器翻译系统。
  - 在整体的系统框架上，还是LSTM RNNs，包括3大块：encoder network, decoder network和attention network。
  - Google通过一些实验也验证了先前论文的结论——神经网络的层数越深，模型的效果越好。当然为了达到这目标，在普通的stacked LSTM上需要加上residual connections，否则会因为exploding and vanishing gradient问题，在6层左右效果达到最好，层数再加深时效果反而会变差。
  - 在attention network方面，用上普通的含有单个隐含层的神经网络，外加softmax层。
  - 在模型训练方面，考虑到计算量大，同时用上了模型并行和数据并行。每个replica使用融合Adam和SGD后的异步更新算法，外加mini-batch。
  - 在处理OOV这个细节问题上，使用wordpiece model。这个模型单独训的，个人觉得也没必要jointly training，因为单独训的话可以使用丰富的单语语料。给定单语语料和一份定好的token集合，基于MLE即可得到一个wordpiece model。
  - 在优化目标方面，最大化给定input计算ground-truth output的概率这种目标与最终的翻译质量评估指标并不完全吻合，所以提出expected reward objective，同时融合MLE和expected reward objective，以期在max likelihood和BLEU（GLEU）上达到平衡。
  - 在工程实现方面，在处理浮点数和提高计算并发度等方面，也做了很多很棒的工作。

+ [Google's Multilingual Neural Machine Translation System: Enabling Zero-Shot Translation](https://arxiv.org/abs/1611.04558)
  - 介绍在GNMT系统方案的基础上，如何实现多语言翻译和zero-shot translation。作者提出了一个简洁的方案，即在源语言句子中增加一个表示目标语言的token。
  - 为此作者做了充分的实验
    + one to many, many to one, many to many
    + 合并全部双语语料，在诸多语言对上只训练一个模型。这个好处可想而知，减少训练开销，减少线上加载模型的内存开销。
    + zero-shot translation
    + 当新语料到来时，用新语料是否还会提高效果。合并数据后重训模型 or incremental training。在作者的试验中，合并数据后重训模型的效果更好。
    + 对于前3项的实验，作者的方案虽然简单但都是有效的。
  - 可视化翻译效果，对属于同一语系的相似语言，可以看到一些纯正的cluster。
  - Mixed language。这个也算是作者在玩语言玩数据了
  - 个人觉得这篇文章虽然idea不是特别高深，但其实验结果的价值还是蛮大的，也许能引起语言学界特别翻译界的学者的后续深入研究。也许在寻找interlingua这个问题上，这种方法这种思路是不错的。

## 深度学习 (Deep Learning, DL)

+ [Deep Learning Tutorial](http://www.slideshare.net/tw_dsconf/ss-62245351)  来自国立台湾大学的李宏毅
  - 这是一篇关于DL非常非常好的tutorial，适合需要入门DL的同学看。讲稿里介绍了DL的多个基础细节以及CNN和RNN，几乎对每一个概念都用图来表达，对一些trick的效果都摘取了paper的实验结果。
  - 有了这个基础后，可以另外找资料深入了解感兴趣的细节，比如求解算法（如BPTT）的推导细节和各个trick背后的数学基础（比如CNN的卷积层和pooling层既充当了提炼图像特征的功能，又有效降低通常全连接NN网络结构的参数规模。又比如dropout为啥有ensemble的效果。再比如RNN怎么用到各种NLP问题中）等。

## 职业规划

互联网的热是好事也是坏事。一方面，处处是机会，工程师基本不愁找不着工作，尤其是算法工程师基本不愁找不着好工作。另一方面，也处处是浮躁，这个时代的程序员可以面向工资编程了，待遇高自然是好事，但千万不能被待遇蒙蔽了双眼。职业生涯很长，码农们要学会规划自己，基于自己的优缺点梳理好自己的短期目标和长期目标，在每个阶段都不要局限自己的视野，而要放大自己的格局。

最近看到几篇文章，觉得还不错，索性就把网址贴到这里。

+ [技术人员的发展之路](http://coolshell.cn/articles/17583.html) 来自 CoolShell博客
  - 可能是因为自己也在考虑职业规划，所以觉得这篇文章可谓字字珠玑

+ [广告算法工程师的核心竞争力是什么？](https://www.zhihu.com/question/41081387) 来自知乎
  - 业务和算法是算法工程师的两条腿，两条腿都要走都要硬。

* * *

**原创文章，转载请注明：**转载自[{{ site.title }}]({{ site.production_url }})

**本文链接地址：**[{{ page.title }}]({{ site.production_url }}{{ page.url }})

* * *
