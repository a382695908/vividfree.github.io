---
layout: post
title: 201706 阅读笔记
category: 阅读笔记
tagline: 
tags: [阅读笔记, dropout, 深度学习, 机器翻译, attention]
theme :
  name : bootstrap-3
---
{% include JB/setup %}

## dropout

Dropout是深度学习里一种常用的正则化方法，它计算方便但功能强大，并可以与L2参数范数惩罚、max-norm等正则化技术组合一起提高模型的泛化能力。
+ [Dropout: A simple way to prevent neural networks from overfitting](https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf)
  - Hinton组发表的paper，dropout方面最经典的文章，从bagging集成模型的角度介绍dropout是一种正则化方法。
  - Dropout方法受生物学的想法启发：有性繁殖涉及到两个不同生物体之间交换基因，进化产生的压力使得基因不仅是良好的而且要准备好不同有机体之间的交换。这样的基因和这些特点对环境的变化时非常稳健的，因为它们一定会正确适应任何一个有机体或模型不寻常的特性。因此dropout正则化每个隐藏单元不仅是一个很好的特征，更要在许多情况下是良好的特征。通俗点讲就是，网络中每个节点的学习不要假设兄弟节点能够一直稳定的运行，这就要求各节点要更“努力”些，具有更强的能力。
  - 在实际实现dropout时，一般会通过引入服从伯努利分布的随机mask来做，这个mask与激活函数的输出值相乘，如果mask的随机采样值为0，那么在正向传播（inference）和误差反向传播更新权重（BP算法）时，该节点既不能带来分数上的贡献，而且其相关的边权重也不会被修改。
  - 本文最精彩篇幅最大的地方就是在横跨 图像、语音、文本、基因 等多个数据集上取得了 当时的 state-of-the-art 结果或者次优解，足见这个方法的厉害之处。
  - 还把dropout与bayesian neural networks对比。依贝叶斯理论，模型参数是有先验概率的，不同参数对应不同的模型，它们可以有不同的先验概率。贝叶斯理论用后验概率对问题建模。在这篇文章的实验中，dropout的效果并不比bayesian neural networks的效果差多少，但NN with dropout的训练和预测的效果都快很多。
  - 通过在验证集做实验，提出dropout的p（节点的保留概率）取值在0.5到0.8之间时模型的效果会比较好。
+ [Dropout as data augmentation](https://arxiv.org/abs/1506.08700)
  - 上一篇文章是从bagging集成方法的角度来介绍dropout可以防止过拟合，而这篇文章则是从数据增强的角度来介绍（在图像处理领域，数据增强是一种常用的正则化方法）。
  - 文章的总体工作是：对于每一个dropout后的网络，进行训练时，相当于做了data augmentation，因为总可以找到一个样本，使得在原始的网络上也能达到dropout单元后的效果。 
+ [Deep Learning](http://www.deeplearningbook.org/)
  - 目前深度学习领域最深入的一本书，一共3位作者，他们是 Ian Goodfellow and Yoshua Bengio and Aaron Courville
  - 其中的第7.12节介绍了dropout，主要是站在baggging角度系统总结了dropout的多种优点及其变种。

Dropconnect是LaCun组提出的一种对dropout的简单改进。这2种技术都用来防止过拟合，它们的区别大体在于，在训练过程中dropout是随机drop掉一些节点，而dropconnect则是随机drop掉一些边。因为目前实际中用dropout技术更多些，所以暂时没看dropconnect技术细节，这里就附上一份博文，将来再详细了解下。
+ [Dropout 与 DropConnect](http://www.voidcn.com/blog/losteng/article/p-5991164.html)

## Machine Translation

上个月Facebook发表《Convolutional sequence to sequence learning》，提出用CNN做机器翻译的模型（ConvS2S），模型效果比去年Google基于LSTM做的机器翻译模型（GNMT）的效果更好。没想到这么快，Google又提出一个效果更好的模型，这个模型本身并不复杂，但想法更大胆更直接，不用CNN和RNN，全部用attention。用attention捕获远距离依赖，能捕获全局信息。

+ [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
  - 提出 scaled dot-product attention 和 multi-head attention 。这两种是整个encoder-decoder模型（作者形容为Transformer模型，参见图1）的核心，也是本文的创新之处。此外，框架还包括residual connection，layer normalization, 全连接层，positional encoding等常用模块。在正则化方面，使用了3种正则化技术，residual dropout（节点额保留概率配置为0.9），attention dropout 和 label smoothing 。
  - 上述2种attention自身不带要学习的参数，仅仅是有若干需要在验证集上调试的超参数。过去一些NMT模型中的attention是需要建立神经网络模型，会引入更多的要学习的参数。所以有了这对比，足见Transformer模型的简洁。
  - 文章还从 complexity per layer 和 maximum path length 等方面说明 self-attention 是简单可依赖的，有关这块的详情可以阅读paper的第4节及表1的数据。

<div align="center">
  <img src="/images/2017-06-24-201706-reading-list-figure1.jpg" style="max-width:393px; text-align:center" alt=""/>
</div>

* * *

**原创文章，转载请注明：**转载自[{{ site.title }}]({{ site.production_url }})

**本文链接地址：**[{{ page.title }}]({{ site.production_url }}{{ page.url }})

* * *
