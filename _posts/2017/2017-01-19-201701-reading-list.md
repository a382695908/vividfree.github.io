---
layout: post
title: 201701 阅读笔记
category: 阅读笔记
tagline: 
tags: [阅读笔记, CNN, ReLU]
theme :
  name : bootstrap-3
---
{% include JB/setup %}

## 技术方面

+ [为什么使用 ReLU](http://shuokay.com/2016/10/01/why-relu-work/)
  - 在深度学习尤其是卷积神经网络中, 用的最多的是 ReLU 函数。
  - 直观上看, ReLU 是一个分段的线性函数
  - 提到一种理解ReLU的方案：类比一下 Boost 算法, 把 ReLU 看成是一个分段的, 用来 separate 数据的函数, 而不是一个去真正的拟合某个函数. 在机器学习中, 数据集都是有限的, 所以, 通过去分割数据空间, 只要分割的次数足够多, 那么, 总是可以得到正确的分割结果.

+ [请问人工神经网络中的activation function的作用具体是什么？为什么ReLu要好过于tanh和sigmoid function?](https://www.zhihu.com/question/29021768)
  - ReLU的优点：发现ReLU更容易学习优化。因为其分段线性性质，导致其前传，后传，求导都是分段线性。而传统的sigmoid函数，由于两端饱和，在传播过程中容易丢弃信息。
  - 缺点是不能用Gradient-Based方法。同时如果de-active了，容易无法再次active。不过有办法解决，使用maxout激活函数

+ [Yoshua Bengio为什么能跟Hinton、LeCun相提并论？](https://www.zhihu.com/question/37922364)
  - 了解大神们都有多大的贡献

* * *

**原创文章，转载请注明：**转载自[{{ site.title }}]({{ site.production_url }})

**本文链接地址：**[{{ page.title }}]({{ site.production_url }}{{ page.url }})

* * *
