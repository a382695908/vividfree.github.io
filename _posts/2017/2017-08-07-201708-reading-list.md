---
layout: post
title: 201708 阅读笔记
category: 阅读笔记
tagline: 
tags: [阅读笔记, product quantization, 乘积量化, similarity search, 相似搜索, nearest neighbor search, 近邻搜索, tensorflow, deep learning, 发展趋势, 白化, PCA白化, ZCA白化]
theme :
  name : bootstrap-3
---
{% include JB/setup %}

## Product quantization 乘积量化

Product quantization算法可以用在相似搜索和模型压缩等问题上。对在大数据量高维度数据集上的相似搜索问题，product quantization算法的效果和性能都是不错的。前几天就product quantization算法写了些个人理解《[理解 product quantization 算法](http://vividfree.github.io/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2017/08/05/understanding-product-quantization)》

## TensorFlow for Deep Learning Research

[TensorFlow for Deep Learning Research](http://web.stanford.edu/class/cs20si/syllabus.html) 是Stanford大学新开的一门课程
+ 虽然TensorFlow已经成为最火热的开源深度学习工具包，但笔者着实没想到Stanford竟会专门开门课介绍TensorFlow。TF背靠Google，俨然快成为新时代的基础工具包，难怪之前业内某知名人士呼吁避免使用TF，而使用更加开放中立的开源系统，比如Caffe, MxNet等。因为商业竞争，大厂需要建立护城河，那么是应该重视此问题，不过对于DL小蚂蚁来说，能够帮助快速了解DL技术及其效果的工具自然还是多用用为好。
+ 对于对TF不熟的同学来说，该课程的讲义是一份很不错的资料，值得认真阅读。

## 深度学习的发展趋势

+ 英文原文 [10 Deep Learning Trends and Predictions for 2017](https://medium.com/intuitionmachine/10-deep-learning-trends-and-predictions-for-2017-f28ca0666669)
  - 英文原文写于16年年底，中文翻译请见 [深度学习在2017年的十大发展趋势及预测](http://geek.ai100.com.cn/2017/02/14/445)
  - 文章列了10点趋势判断，这里就简单摘记如下。像其中的第2点，今年上半年Facebook就发表了用CNN做MT的paper，从paper的结果看，其效果超过Google之前的用RNN做MT的效果。
    - 硬件将双倍加速摩尔定律
    - 卷积神经网络（CNN）将占主导地位
    - 设计者会更依赖 Meta-learning
    - 强化学习仅会变得更具创造性
    - 对抗与合作学习将称王
    - 预测学习或者无监督学习的进展不会太多
    - 从学习到工业化的转化
    - 深度学习将会成为更多应用的组成部分
    - 采用设计模式的频率会越来越高
    - 工程化速度将超过理论出现的速度
    
## 白化、PCA白化、ZCA白化

来自UFLDL的资料
+ [白化](http://ufldl.stanford.edu/wiki/index.php/%E7%99%BD%E5%8C%96)
+ [实现主成分分析与白化]( http://ufldl.stanford.edu/wiki/index.php/%E5%AE%9E%E7%8E%B0%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E5%92%8C%E7%99%BD%E5%8C%96)

其他若干资料
+ [白化whitening](http://blog.csdn.net/hjimce/article/details/50864602)
+ [白化（Whitening） PCA白化 ZCA白化](http://blog.csdn.net/danieljianfeng/article/details/42147109)
+ [白化（Whitening）：PCA vs. ZCA](https://my.oschina.net/findbill/blog/543485)

简要总结下（推导和具体的算法流程比较简单，请见上面这些文章，这里就不赘述）：
+  在一些算法中需要白化这样一个数据预处理步骤。举例来说，假设训练数据是图像，由于图像中相邻像素之间具有很强的相关性，所以用于训练时输入是冗余的。白化的目的就是降低输入的冗余性；更正式的说，我们希望通过白化过程使得学习算法的输入具有如下性质：（1）特征之间相关性较低；（2）所有特征具有相同的方差。
+  PCA变换能完成第1个目的，即让特征之间变得正交，也就是降低了特征之间的相关性。在PCA变换的基础上再让各特征的方差相同，这就是PCA白化。所以PCA白化的结果是让不同的特征之间不相关且都具有单位方差
+ ZCA白化是在PCA白化的基础上再多了一个处理，乘以矩阵U（由PCA分解得到的），将数据又变化回原始空间。
+  为什么要有ZCA白化呢？事实证明这也是一种生物眼睛(视网膜)处理图像的粗糙模型。具体而言，当你的眼睛感知图像时，由于一幅图像中相邻的部分在亮度上十分相关，大多数临近的“像素”在眼中被感知为相近的值。因此，如果人眼需要分别传输每个像素值（通过视觉神经）到大脑中，会非常不划算。取而代之的是，视网膜进行一个与ZCA中相似的去相关操作(这是由视网膜上的ON-型和OFF-型光感受器细胞将光信号转变为神经信号完成的)。由此得到对输入图像的更低冗余的表示，并将它传输到大脑。


* * *

**原创文章，转载请注明：**转载自[{{ site.title }}]({{ site.production_url }})

**本文链接地址：**[{{ page.title }}]({{ site.production_url }}{{ page.url }})

* * *
