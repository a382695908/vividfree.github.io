---
layout: post
title: 201612 阅读笔记
category: 阅读笔记
tagline: 
tags: [阅读笔记, RNN, LSTM, Attention, 机器翻译, 语言模型]
theme :
  name : bootstrap-3
---
{% include JB/setup %}

## 技术方面

虽然之前了解过一些RNN模型，但对其中的很多细节（比如BPTT算法、LSTM的状态计算公式）的理解甚浅。最近又细读了一些资料，受益匪浅。下面是几篇非常好的RNN入门资料，希望后面能在RNN上做些实践做出一些有意思的工作。

+ [RECURRENT NEURAL NETWORKS TUTORIAL, PART 1 – INTRODUCTION TO RNNS](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-1-introduction-to-rnns/)
  - 介绍什么是RNN (recurrent neural networks)
  - 适合处理含有序列结构的问题，像自然语言理解里的很多问题——语言模型，机器翻译，生成图片的描述等。前面这句话反过来说也许更合适，含有序列结构的问题用RNN来建模更为直接，因为序列的上下文信息对建模是有用的，RNN的结构特点使得其能够包含上下文信息。
  - 给出 RNN 的结构示意图。对于最朴素的RNN模型，主要要学习的参数有 U, W, V 三个矩阵，各层之间共享 U, W, V 参数。好处：相比通常的深度神经网络而言，要学习的参数更少，模型过拟合的可能性会适当降低。
  - 因为 U, W, V 三类参数是各层共享的，所以在优化算法这块，跟传统神经网络的BP算法有点不同，对RNN而言，优化算法称为BPTT算法。
  - RNN的变种有：Bidirectional RNNs, Deep (Bidirectional) RNNs, LSTM networks
+ [RECURRENT NEURAL NETWORKS TUTORIAL, PART 2 – IMPLEMENTING A RNN WITH PYTHON, NUMPY AND THEANO](http://www.wildml.com/2015/09/recurrent-neural-networks-tutorial-part-2-implementing-a-language-model-rnn-with-python-numpy-and-theano/)
  - 语言模型的问题描述
  - 基于python语言的文本预处理（通常文本处理都需要这些步骤）和训练RNN模型
  - 细节处理：初始化模型参数（在某个区间内按照均匀分布随机采样），检测梯度（该方法在很多文献中都提过，用差分方法逼近微分），使用Theano优化性能
+ [RECURRENT NEURAL NETWORKS TUTORIAL, PART 3 – BACKPROPAGATION THROUGH TIME AND VANISHING GRADIENTS](http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/)
  - 介绍BPTT算法，给出迭代计算公式
  - 介绍梯度消失问题（Vanishing gradient problem）：计算梯度的链式法则使得远距离依赖的特征的梯度很小，造成远距离依赖关系难以学到
  - 对梯度消失问题的解决办法
    + 对 W 矩阵用更好的初始化方法
    + 使用 ReLU 替换 tanh函数 或者 sigmoid函数
    + 使用 Long Short-Term Memory (LSTM) 或者 Gated Recurrent Unit (GRU) 结构
+ [RECURRENT NEURAL NETWORKS TUTORIAL, PART 4 – IMPLEMENTING A GRU/LSTM RNN WITH PYTHON AND THEANO](http://www.wildml.com/2015/10/recurrent-neural-network-tutorial-part-4-implementing-a-grulstm-rnn-with-python-and-theano/)
  - 介绍 LSTM：含有 i, f, o 三种gate，分别表示input, forget and output gate。含有 隐藏状态 s 和 记忆 c。
  - 介绍 GRU：含有 r, z 两种gate，分别表示reset gate 和 update gate。
  - LSTM和GRU的效果对比，暂时还难分伯仲，更多的时候学者是在调试网络层数、节点数等网络结构。GRU的模型参数少，如果样本量充足的话，可以用LSTM，毕竟LSTM的模型表达能力更强。
  - SGD的变种（这点在之前的"201603 阅读笔记"博客中也提到过）

+ [Understanding LSTM Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  - 相比 "RECURRENT NEURAL NETWORKS TUTORIAL"系列的4篇文章而言，这篇文章用更形象更具体的语言描述了LSTM的网络结构

+ [ATTENTION AND MEMORY IN DEEP LEARNING AND NLP](http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/)
  - 介绍attention机制。引入attention机制到机器翻译问题中，能同时把词语对齐问题和翻译问题一同被模型处理，也就是论文《Neural Machine Translation by Jointly Learning to Align and Translate》标题上的jointly learning。
  - 介绍memory机制：可以理解attention机制是一种特殊的memory机制。
+ [ATTENTION MECHANISM](https://blog.heuritech.com/2016/01/20/attention-mechanism/)
  - 这篇文章用形象的图描述了attention机制在机器翻译、图片描述等问题的应用。
+ [Attention and Augmented Recurrent Neural Networks](http://distill.pub/2016/augmented-rnns/)


* * *

**原创文章，转载请注明：**转载自[{{ site.title }}]({{ site.production_url }})

**本文链接地址：**[{{ page.title }}]({{ site.production_url }}{{ page.url }})

* * *
