---
layout: post
title: 201603 阅读笔记
category: 阅读笔记
tagline: 
tags: [阅读笔记]
theme :
  name : bootstrap-3
---
{% include JB/setup %}

## 技术方面

+ [基于深度学习的机器翻译研究进展](http://vdisk.weibo.com/s/FGSzrtQf1Xgz) . 作者: 刘洋 

+ [深度学习与自然语言处理之五：从RNN到LSTM](http://blog.csdn.net/malefactor/article/details/50436735) . 作者: 张俊林

+ [CS 224D: Deep Learning for NLP](http://cs224d.stanford.edu/) . 作者: Richard Socher

+ [Deep Learning for NLP - Tips and Techniques](http://www.academia.edu/17684773/DEEP_LEARNING_FOR_NLP_-_TIPS_AND_TECHNIQUES) . 作者: Moloy De
  - 在80页PPT中介绍了相关的内容，可以当做科普知识
  - 先列举了一些ML, DL, NLP的基本概念，比如: language model, softmax function, gradient descent(提了下AdaGrad), CBOW, Skip-Gram, negative sampling, cross-entropy loss(log loss)
  - 然后介绍了neural network中的一些内容
    + 激活函数(neuron unit): sigmoid, tanh, hard tanh, soft sign, rectified linear neuron unit, leaky rectified linear neuron unit
    + Recurrent Neural Networks(RNN): 最朴素的RNN模型, deep bidirectional RNNs。在激活函数这块，再介绍了 Gated Recurrent Units(GRU), Long-Short-Term-Memories(LSTM)

+ [An overview of gradient descent optimization algorithms](http://sebastianruder.com/optimizing-gradient-descent/)
  - 关于 梯度下降法（Gradient Descent）的很好的资料
  - 梯度下降的3个变种：batch, stochastic, mini-batch（它们的优缺点比较好理解）
  - 提出GD方法的4个挑战
    + 如何选取合适的learning rate
    + 如何在迭代计算中调整learning rate
    + 是否可以给不同维度用上不同的learning rate
    + 如何快速跳出鞍点(saddle points)
  - 围绕上面4个挑战，介绍了多种算法（当然这些算法不是都能处理上面4种挑战）
    + 动量法(Momentum): 加了一项，或者可以认为修正方向是用指数平滑法计算出
    + Nesterov accelerated gradient: 在动量法的基础上再考虑修正后到达位置的梯度，可以初略认为是让算法快到达极小点时能一定程度调整好方向。
    + Adagrad: 上面2种方法没有分维度给出不同的learning rate，而该方法就希望做到这点，大体方针是调整多的维度的learning rate可以小点，调整少的维度的learning rate可以大点。FRTL算法里也用了类似的思路（有小点不同）。不足之处在于：在迭代次数达到一定次数后，learning rate的值会很小，使得模型基本不能更新了
    + Adadelta: 在Adagrad基础上增加了一点因素。好处：没有超参数了，不用拍定一个默认的learning rate
    + RMSprop: 在Adagrad的基础上，设置默认的 gama 值为0.9
    + Adam: 在计算leanring rate时，不但考虑梯度的平方，还考虑梯度。
  - 给了2个示意图，可以看到 Adagrad, Adadelta, RMSprop 这三个方法可以快速收敛；另外在“如何选择算法”一节中也推荐了在一些场景下可以用Adam 
  - 如何并行化：对于稀疏模型可以用Hogwild算法（比如CTR预估里如果用到很多ID类特征就可以用这种方法），Downpour SGD, Delay-tolerant Algorithms for SGD等算法
  - 其他细节：shuffling, batch normalization(每过一定迭代轮数就做一次参数归一化), Early stopping（一种减少过拟合的简单方法，分出一个验证集，每过一定迭代轮数就看下验证集上的error是否有变化，如果没有变化或者出现上升则停止迭代）

## 行业观察

+ Google研发的AlphaGo以4:1的比分击败九段棋手李世石
+ Google挂牌出售旗下的Boston Dynamics


* * *

**原创文章，转载请注明：**转载自[{{ site.title }}]({{ site.production_url }})

**本文链接地址：**[{{ page.title }}]({{ site.production_url }}{{ page.url }})

* * *
